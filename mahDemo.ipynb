{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Helper Functions (not used in example)\n",
    "\n",
    "These functions are helpful to log the prompt used, or format the response so it readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def markdown_to_html(markdown_text):\n",
    "    html = Markdown(markdown_text)\n",
    "    display(html)\n",
    "\n",
    "def log_prompt(system, user, format):\n",
    "    markdown_to_html(\"# PROMPT\")\n",
    "    markdown_to_html(\"## system\")\n",
    "    print(system)\n",
    "    markdown_to_html(\"## user\")\n",
    "    print(user)\n",
    "    markdown_to_html(\"## format\")\n",
    "    print(format)\n",
    "\n",
    "def log_request(model):\n",
    "    print(f\"# BEGIN {model}\\n\")\n",
    "\n",
    "def log_response(response, model = \"\"):\n",
    "    print(f\"# END {model}\\n\")\n",
    "    markdown_to_html(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call multiple AI's by sending a list of model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MultiAiHub as mah\n",
    "\n",
    "# Possible Models to use:\n",
    "# ANTHROPIC_OPUS = \"claude-3-opus-20240229\"\n",
    "# ANTHROPIC_SONNET = \"claude-3-sonnet-20240229\"\n",
    "# GEMINI_PRO = \"gemini-pro\"\n",
    "# OPEN_AI_GPT35TURBO = \"gpt-3.5-turbo\"\n",
    "# OPEN_AI_GPT4 = \"gpt-4\"\n",
    "# OPEN_AI_GPT4PREVIEW = \"gpt-4-0125-preview\"\n",
    "# MISTRAL_7B = \"mistral-7b-instruct\"\n",
    "# MIXTRAL_8X7B = \"mixtral-8x7b-instruct\"\n",
    "# SONAR_MED_ONLINE = \"sonar-medium-online\"\n",
    "models = [  \n",
    "    mah.ANTHROPIC_SONNET,\n",
    "    mah.ANTHROPIC_OPUS,\n",
    "    mah.GEMINI_PRO,\n",
    "    mah.OPEN_AI_GPT35TURBO,\n",
    "    mah.MIXTRAL_8X7B\n",
    "]\n",
    "\n",
    "system = \"You are a pirate\"\n",
    "user = \"Make a greeting and tell me a joke about treasure\"\n",
    "format = \"Output the response in all capital letters\"\n",
    "\n",
    "response = mah.generate_text(models, system, user, format)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call a single API, and get a raw response without the model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MultiAiHub as mah\n",
    "\n",
    "# Only 1 model in the list\n",
    "models = [    \n",
    "    mah.GEMINI_PRO,\n",
    "]\n",
    "\n",
    "system = \"You are a pirate\"\n",
    "user = \"Make a greeting and tell me a joke about treasure\"\n",
    "format = \"Output the response in all capital letters\"\n",
    "\n",
    "# Will call all the models and return with a single response\n",
    "response = mah.generate_text(models, system, user, format)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
