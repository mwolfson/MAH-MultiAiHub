{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Helper Functions (not used in example)\n",
    "\n",
    "These functions are helpful to log the prompt used, or output_style the response so it readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_md_to_html(markdown_text):\n",
    "    html = Markdown(markdown_text)\n",
    "    display(html)\n",
    "\n",
    "def save_response_to_file(file_text):\n",
    "    # Generate a filename\n",
    "    now = datetime.now()\n",
    "    identifier = \"pirate\"  # Set this to something unique to your prompt\n",
    "    file_type = \"md\"\n",
    "    formatted_date = now.strftime(\"%Y%m%d_%H%M%S\")  # Example: 20231127_153210    \n",
    "    filename = f\"{identifier}_{formatted_date}.{file_type}\"\n",
    "\n",
    "    # Save the file\n",
    "    try:\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(file_text)\n",
    "    except OSError as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "# PROMPT\n",
    "## SYSTEM\n",
    "```markdown\n",
    "{system}\n",
    "```\n",
    "\n",
    "## USER\n",
    "```markdown\n",
    "{user}\n",
    "```\n",
    "\n",
    "## OUTPUT STYLE\n",
    "```markdown\n",
    "{output_style}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multiple APIs** - Call multiple AI's by sending a list of model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multi_ai_hub as mah\n",
    "\n",
    "# Possible Models to use:\n",
    "# ANTHROPIC_OPUS = \"claude-3-opus-20240229\"\n",
    "# ANTHROPIC_SONNET = \"claude-3-sonnet-20240229\"\n",
    "# GEMINI_PRO = \"gemini-pro\"\n",
    "# HUGGINGFACE_BLOOM = \"bigscience/bloom\"\n",
    "# HUGGINGFACE_GEMMA7B = \"google/gemma-7b\"\n",
    "# OPEN_AI_GPT35TURBO = \"gpt-3.5-turbo\"\n",
    "# OPEN_AI_GPT4 = \"gpt-4\"\n",
    "# OPEN_AI_GPT4PREVIEW = \"gpt-4-0125-preview\"\n",
    "# MISTRAL_7B = \"mistral-7b-instruct\"\n",
    "# MIXTRAL_8X7B = \"mixtral-8x7b-instruct\"\n",
    "# SONAR_MED_ONLINE = \"sonar-medium-online\"\n",
    "models = [  \n",
    "    mah.ANTHROPIC_SONNET,\n",
    "    mah.GEMINI_PRO,\n",
    "    mah.HUGGINGFACE_GEMMA7B\n",
    "]\n",
    "\n",
    "system = \"You are a pirate\"\n",
    "user = \"Make a greeting and tell me a joke about treasure\"\n",
    "output_style = \"Output the response in all capital letters\"\n",
    "\n",
    "response = mah.generate_text(models, system, user, output_style)\n",
    "\n",
    "# Output the response to the console\n",
    "print(response)\n",
    "\n",
    "# Optionally, use the custom logging functions to display and save the response\n",
    "# prompt = prompt_template.format(system=system,\n",
    "#                                 user=user,\n",
    "#                                 output_style=output_style)\n",
    "# display_md_to_html(prompt + response)\n",
    "# save_response_to_file(prompt + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call **Single Model**, and get a raw response without the model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multi_ai_hub as mah\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    mah.OPEN_AI_GPT35TURBO,\n",
    "    mah.NVIDIA_LLAMA3_70B\n",
    "]\n",
    "\n",
    "system = \"You are a pirate\"\n",
    "user = \"Make a greeting and tell me a joke about treasure\"\n",
    "output_style = \"Output the response in all capital letters\"\n",
    "\n",
    "# Will call all the models and return with a single response\n",
    "response = mah.generate_text(models, system, user, output_style)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
