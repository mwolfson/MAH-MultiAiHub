{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e564a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94215c9f",
   "metadata": {},
   "source": [
    "## MAH - Multi AI Hub\n",
    "\n",
    "This project is designed to make it easy to send the same prompt to multiple LLMs which is useful for testing and comparison.\n",
    "\n",
    "### API Access Required\n",
    "\n",
    "You must have access to the services (Currently Anthropic, Google, OpenAI, and Perplexity) in order to use them in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fde748",
   "metadata": {},
   "source": [
    "### Working with API keys\n",
    "\n",
    "Set the API keys as a system variables.\n",
    "\n",
    "- [Setting an Environment Variable on Mac/Linux](https://phoenixnap.com/kb/set-environment-variable-mac)\n",
    "- [Setting an Environment Variable on Windows](https://phoenixnap.com/kb/windows-set-environment-variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c7f07",
   "metadata": {},
   "source": [
    "## Tools to Get Environment Variables from OS\n",
    "\n",
    "PIP Install:\n",
    "\n",
    "`pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbf9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d788df",
   "metadata": {},
   "source": [
    "## Adding other models\n",
    "\n",
    "### Check for Provider *Helper* Function\n",
    "\n",
    "This is organized into API providers, there are helper functions for:\n",
    "- [**Anthropic**](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)\n",
    "- [**Google**](https://ai.google.dev/)\n",
    "- [**Hugging Face**](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client)\n",
    "- [**OpenAI**](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)\n",
    "- [**Perplexity**](https://docs.perplexity.ai/)\n",
    "\n",
    "Create a new helper function if necessary, then skip to the bottom, and add your calls to the Action dictionary, where these are mapped (pretty simple)\n",
    "\n",
    "Happy Model Comparing!\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85524dc3",
   "metadata": {},
   "source": [
    "## Setup Google GenAI\n",
    "\n",
    "### Import Google Generative GenerativeAI library and set API Key\n",
    "\n",
    "PIP Install: \n",
    "\n",
    "`pip install -q google.generativeai`\n",
    "\n",
    "You will need to set the Gemini API key as a system variable named: `GOOGLE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27165c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as googleai\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "apiKey = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "googleai.configure(api_key=apiKey,\n",
    "               transport=\"rest\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20290bb8",
   "metadata": {},
   "source": [
    "## Explore the Available Models\n",
    "\n",
    "Learn which models are currently available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f805008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in googleai.list_models():\n",
    "#     print(f\"name: {m.name}\")\n",
    "#     print(f\"description: {m.description}\")\n",
    "#     print(f\"generation methods:{m.supported_generation_methods}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e486139",
   "metadata": {},
   "source": [
    "### Filter models to ensure model we want is supported\n",
    "- `generateContent` is the value we are looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d50897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in googleai.list_models():\n",
    "#   if 'generateContent' in m.supported_generation_methods:\n",
    "#     print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3b4ba",
   "metadata": {},
   "source": [
    "### Google AI Helper Function\n",
    "\n",
    "- The `@retry` decorator helps you to retry the API call if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8738a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "@retry.Retry()\n",
    "def generate_text_google(prompt, model):\n",
    "    model = googleai.GenerativeModel(model)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb2c16",
   "metadata": {},
   "source": [
    "### Test **Google AI Helper** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c0331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generate_text_google(\"Thursday evenings are perfect for\", \"gemini-pro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0bb07",
   "metadata": {},
   "source": [
    "## Setup Open AI APIs\n",
    "\n",
    "```\n",
    "OpenAI's APIs offer developers the ability to integrate advanced artificial intelligence capabilities into their applications, enabling a wide range of tasks from text generation to complex problem-solving.\n",
    "```\n",
    "Documentation: [https://beta.openai.com/docs/](https://beta.openai.com/docs/)\n",
    "\n",
    "### Obtaining API Keys:\n",
    "- **OpenAI Platform**: [https://platform.openai.com/](https://platform.openai.com/)\n",
    "  - After signing up or logging in, navigate to the API section to manage and obtain your API keys.\n",
    "- You will need to set the OpenAI API key as a system variable named: `OPENAI_API_KEY`.  \n",
    "\n",
    "Note: do NOT check your API key into a public Github repo, or it will get revoked \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc17cf19-f3ee-4ccb-95fe-afbf2d592c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ecbad",
   "metadata": {},
   "source": [
    "### Open AI Helper Function\n",
    "\n",
    "PIP Dependencies:\n",
    "\n",
    "`pip install --upgrade openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa30282-118b-499e-978e-39e832214847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_text_openai(pre, prompt, model):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": pre},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39685df4",
   "metadata": {},
   "source": [
    "## Test **Open AI Helper** Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3ebc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generate_text_openai(\"You are a pirate\", \"Thursday evenings are perfect for\", \"gpt-3.5-turbo\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e34e4",
   "metadata": {},
   "source": [
    "## Setup Perplexity API\n",
    "\n",
    "You will need a key set to `PERPLEXITY_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd41cb9b-1e34-45ed-87b2-67465a964237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "YOUR_API_KEY = os.getenv('PERPLEXITY_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b2815",
   "metadata": {},
   "source": [
    "## Perplexity Helper function\n",
    "\n",
    "No PIP dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5594e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "perplexityClient = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "def generate_text_perplexity(system, user, model):\n",
    "    response = perplexityClient.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d978e",
   "metadata": {},
   "source": [
    "## Test **Perplexity Helper** Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generate_text_perplexity(\"you are a pirate\", \"say hello and return the message in uppercase\", \"mistral-7b-instruct\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47dfdb",
   "metadata": {},
   "source": [
    "## Setup Anthropic\n",
    "\n",
    "Check the [docs](https://github.com/anthropics/anthropic-sdk-python), and get an [API Key](https://console.anthropic.com/dashboard)\n",
    "\n",
    "### Import SDK \n",
    "\n",
    "PIP Install:\n",
    "\n",
    "`pip install anthropic`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec702c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "anthropic_client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def generate_text_anthropic(user, model=\"claude-3-opus-20240229\"):\n",
    "    response = anthropic_client.messages.create(\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        )\n",
    "    content = response.content[0].text \n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129c756",
   "metadata": {},
   "source": [
    "### Test the Anthropic API directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af97edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generate_text_anthropic(\"you are a pirate\" + \"say hello and return the message in uppercase\", \"claude-3-opus-20240229\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f2438",
   "metadata": {},
   "source": [
    "## Setup Hugging Face\n",
    "\n",
    "### PIP Install Hugging Face Hub\n",
    "\n",
    "`pip install --upgrade huggingface_hub`\n",
    "\n",
    "### Get API Key\n",
    "\n",
    "Head to HuggingFace [Settings Page](https://huggingface.co/settings/tokens) and create an API token.\n",
    "\n",
    "and set it as an environment variable named: `HUGGING_FACE_HUB_TOKEN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5457ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "HF_API_KEY = os.getenv('HUGGING_FACE_HUB_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6631fe",
   "metadata": {},
   "source": [
    "## Hugging Face Helper function using the InferenceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da854c",
   "metadata": {},
   "source": [
    "### Use the InferenceClient to check if a model is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e6c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigscience/bloom | bigcode/starcoder\n",
    "\n",
    "# from huggingface_hub import InferenceClient\n",
    "# client = InferenceClient()\n",
    "# client.get_model_status(\"bigscience/bloom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17aca3",
   "metadata": {},
   "source": [
    "### Must Enable Models In Hugging Face to use them \n",
    "\n",
    "**Note** - to use HF models (which can be an URL to a private model, or a `model_id`) you will need to load that [model](https://huggingface.co/models) into your Hugging Face profile first.\n",
    "\n",
    "Some models are available without enabling them. The first models includes `bloom` which is already enabling, and `gemma7b` which is one that requires to enable it first before using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21f26589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient, InferenceTimeoutError\n",
    "\n",
    "def generate_text_huggingface(user, model=\"\"):\n",
    "    try:\n",
    "        huggingface_client = InferenceClient(model=model, token=HF_API_KEY, timeout=60)\n",
    "        response = huggingface_client.text_generation(user, max_new_tokens=1024)\n",
    "    except InferenceTimeoutError:\n",
    "        print(\"Inference timed out after 60s.\")\n",
    "    return response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8ff78",
   "metadata": {},
   "source": [
    "### Test the Hugging Face Helper function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba271a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_text_huggingface(\"you are a pirate tell me your favorite color\", \"google/gemma-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e643db",
   "metadata": {},
   "source": [
    "## Add Actions to map to different models and AI providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aa848",
   "metadata": {},
   "source": [
    "1. Define a function for each model you want to test\n",
    "2. Create a constant to reference that model\n",
    "3. Add both to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "282d025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the common interface for all the models\n",
    "# It takes the **system** message, **user** message and the **output style** instructions and calls\n",
    "# the model specific function with those inputs (matching the API signature)\n",
    "def action_anthropic_opus(system, user, output_style):\n",
    "    response = generate_text_anthropic(system + user + output_style, \"claude-3-opus-20240229\")\n",
    "    return response\n",
    "\n",
    "def action_anthropic_sonnet(system, user, output_style):\n",
    "    response = generate_text_anthropic(system + user + output_style, \"claude-3-sonnet-20240229\")\n",
    "    return response\n",
    "\n",
    "def action_gemini_pro(system, user, output_style,):\n",
    "    response = generate_text_google(system + user + output_style, \"gemini-pro\")\n",
    "    return response\n",
    "\n",
    "def action_huggingface_bloom(system, user, output_style,):\n",
    "    response = generate_text_huggingface(system + user + output_style, \"bigscience/bloom\")\n",
    "    return response\n",
    "\n",
    "def action_huggingface_gemma7b(system, user, output_style,):\n",
    "    response = generate_text_huggingface(system + user + output_style, \"google/gemma-7b\")\n",
    "    return response\n",
    "\n",
    "def action_openai_35turbo(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, \"gpt-3.5-turbo\")\n",
    "    return response\n",
    "\n",
    "def action_openai_gpt4(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, \"gpt-4\")\n",
    "    return response\n",
    "\n",
    "def action_openai_gpt4_preview(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, \"gpt-4-0125-preview\")\n",
    "    return response\n",
    "\n",
    "def action_mistral_7b(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, \"mistral-7b-instruct\")\n",
    "    return response\n",
    "\n",
    "def action_mixtral_8x7b(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, \"mixtral-8x7b-instruct\")\n",
    "    return response\n",
    "\n",
    "def action_sonar_medium_online(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, \"sonar-medium-online\")\n",
    "    return response\n",
    "\n",
    "# Constants for the models\n",
    "ANTHROPIC_OPUS = \"claude-3-opus-20240229\"\n",
    "ANTHROPIC_SONNET = \"claude-3-sonnet-20240229\"\n",
    "GEMINI_PRO = \"gemini-pro\"\n",
    "HUGGINGFACE_BLOOM = \"bigscience/bloom\"\n",
    "HUGGINGFACE_GEMMA7B = \"google/gemma-7b\"\n",
    "OPEN_AI_GPT35TURBO = \"gpt-3.5-turbo\"\n",
    "OPEN_AI_GPT4 = \"gpt-4\"\n",
    "OPEN_AI_GPT4PREVIEW = \"gpt-4-0125-preview\"\n",
    "MISTRAL_7B = \"mistral-7b-instruct\"\n",
    "MIXTRAL_8X7B = \"mixtral-8x7b-instruct\"\n",
    "SONAR_MED_ONLINE = \"sonar-medium-online\"\n",
    "\n",
    "# Dictionary mapping models to their respective functions\n",
    "action_dict = {\n",
    "    ANTHROPIC_OPUS: action_anthropic_opus,\n",
    "    ANTHROPIC_SONNET: action_anthropic_sonnet,\n",
    "    GEMINI_PRO: action_gemini_pro,\n",
    "    HUGGINGFACE_BLOOM: action_huggingface_bloom,\n",
    "    HUGGINGFACE_GEMMA7B: action_huggingface_gemma7b,\n",
    "    OPEN_AI_GPT35TURBO: action_openai_35turbo,\n",
    "    OPEN_AI_GPT4: action_openai_gpt4,\n",
    "    OPEN_AI_GPT4PREVIEW: action_openai_gpt4_preview,\n",
    "    MISTRAL_7B: action_mistral_7b,\n",
    "    MIXTRAL_8X7B: action_mixtral_8x7b,\n",
    "    SONAR_MED_ONLINE: action_sonar_medium_online\n",
    "}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e2bf2",
   "metadata": {},
   "source": [
    "## Main Entry Point to call appropriate functions based which are requested in `models` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c6ecd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(models, system, user, output_style):\n",
    "    \"\"\"\n",
    "    Generate text responses from multiple AIs based on **models** in list.\n",
    "\n",
    "    If there is only 1 models in the list, the response will not include the model name.\n",
    "    Otherwise, the response will include the model name as a header of the text generated from each model.\n",
    "\n",
    "    Args:\n",
    "        models (list): A list of model names indicating which ones to run.\n",
    "        system (str): The prompt *system* information to define context.\n",
    "        user (str): The prompt *user* information to describe the question to ask.\n",
    "        output_style (str): The prompt desired *output_style* of the generated text.\n",
    "\n",
    "    Returns:\n",
    "        str: the generated text for all of the models in the input list\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    is_single_model = len(models) == 1\n",
    "\n",
    "    for model in models:\n",
    "        action = action_dict.get(model)\n",
    "        if action:\n",
    "            try:\n",
    "                response = action(system=system, user=user, output_style=output_style)\n",
    "                if not is_single_model:\n",
    "                    output += \"\\n\\n# MODEL: \" + model + \"\\n\"\n",
    "                output += response\n",
    "            except Exception as e:\n",
    "                if not is_single_model:\n",
    "                    output += \"\\n\\n# MODEL: \" + model + \"\\n\"\n",
    "                output += \"Exception\" + str(e)\n",
    "        else:\n",
    "            print(\"No action defined for model: \", model)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6dac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df7265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
