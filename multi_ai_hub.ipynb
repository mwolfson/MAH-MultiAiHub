{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e564a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94215c9f",
   "metadata": {},
   "source": [
    "## MAH - Multi AI Hub\n",
    "\n",
    "This project is designed to make it easy to send the same prompt to multiple LLMs which is useful for testing and comparison.\n",
    "\n",
    "### API Access Required\n",
    "\n",
    "You must have access to the services (Currently Anthropic, Google, OpenAI, and Perplexity) in order to use them in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fde748",
   "metadata": {},
   "source": [
    "### Working with API keys\n",
    "\n",
    "Set the API keys as a system variables.\n",
    "\n",
    "- [Setting an Environment Variable on Mac/Linux](https://phoenixnap.com/kb/set-environment-variable-mac)\n",
    "- [Setting an Environment Variable on Windows](https://phoenixnap.com/kb/windows-set-environment-variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c7f07",
   "metadata": {},
   "source": [
    "## Tools to Get Environment Variables from OS\n",
    "\n",
    "PIP Install:\n",
    "\n",
    "`pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d788df",
   "metadata": {},
   "source": [
    "## Adding other models\n",
    "\n",
    "### Check for Provider *Helper* Function\n",
    "\n",
    "This is organized into API providers, there are helper functions for:\n",
    "- [**Anthropic**](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)\n",
    "- [**Google**](https://ai.google.dev/)\n",
    "- [**Hugging Face**](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client)\n",
    "- [**OpenAI**](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)\n",
    "- [**NVidia**](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)\n",
    "- [**Perplexity**](https://docs.perplexity.ai/)\n",
    "\n",
    "Create a new helper function if necessary, then skip to the bottom, and add your calls to the Action dictionary, where these are mapped (pretty simple)\n",
    "\n",
    "Happy Model Comparing!\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85524dc3",
   "metadata": {},
   "source": [
    "## Setup Google GenAI\n",
    "\n",
    "### Import Google Generative GenerativeAI library and set API Key\n",
    "\n",
    "PIP Install: \n",
    "\n",
    "`pip install -q google.generativeai`\n",
    "\n",
    "You will need to set the Gemini API key as a system variable named: `GOOGLE_API_KEY`.\n",
    "\n",
    "#### Then load the key from OS and set it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27165c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import google.generativeai as googleai\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "apiKey = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "googleai.configure(api_key=apiKey,\n",
    "               transport=\"rest\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c2dc1c",
   "metadata": {},
   "source": [
    "## Setup Gemini configuration\n",
    "\n",
    "This is where you can configure temperature, safety settings, max tokens, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ed21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold, GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=8192,\n",
    ")\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20290bb8",
   "metadata": {},
   "source": [
    "## Explore the Available Models\n",
    "\n",
    "Learn which models are currently available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f805008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in googleai.list_models():\n",
    "#     print(f\"name: {m.name}\")\n",
    "#     print(f\"description: {m.description}\")\n",
    "#     print(f\"generation methods:{m.supported_generation_methods}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e486139",
   "metadata": {},
   "source": [
    "### Filter models to ensure model we want is supported\n",
    "- `generateContent` is the value we are looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d50897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in googleai.list_models():\n",
    "#   if 'generateContent' in m.supported_generation_methods:\n",
    "#     print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3b4ba",
   "metadata": {},
   "source": [
    "### Google AI Helper Function\n",
    "\n",
    "- The `@retry` decorator helps you to retry the API call if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8738a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "@retry.Retry()\n",
    "def generate_text_google(prompt, model):\n",
    "    model = googleai.GenerativeModel(model_name=model,\n",
    "                              generation_config=generation_config,\n",
    "                              safety_settings=safety_settings)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb2c16",
   "metadata": {},
   "source": [
    "### Test **Google AI Helper** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c0331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generate_text_google(\"Thursday evenings are perfect for\", \"gemini-1.5-flash-latest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0bb07",
   "metadata": {},
   "source": [
    "## Setup Open AI APIs\n",
    "\n",
    "```\n",
    "OpenAI's APIs offer developers the ability to integrate advanced artificial intelligence capabilities into their applications, enabling a wide range of tasks from text generation to complex problem-solving.\n",
    "```\n",
    "Documentation: [https://beta.openai.com/docs/](https://beta.openai.com/docs/)\n",
    "\n",
    "### Obtaining API Keys:\n",
    "- **OpenAI Platform**: [https://platform.openai.com/](https://platform.openai.com/)\n",
    "  - After signing up or logging in, navigate to the API section to manage and obtain your API keys.\n",
    "- You will need to set the OpenAI API key as a system variable named: `OPENAI_API_KEY`.  \n",
    "\n",
    "Note: do NOT check your API key into a public Github repo, or it will get revoked \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc17cf19-f3ee-4ccb-95fe-afbf2d592c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ecbad",
   "metadata": {},
   "source": [
    "### Open AI Helper Function\n",
    "\n",
    "PIP Dependencies:\n",
    "\n",
    "`pip install --upgrade openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa30282-118b-499e-978e-39e832214847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_text_openai(pre, prompt, model):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": pre},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39685df4",
   "metadata": {},
   "source": [
    "## Test **Open AI Helper** Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3ebc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate_text_openai(\"You are a pirate\", \"Thursday evenings are perfect for\", \"gpt-4o\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e34e4",
   "metadata": {},
   "source": [
    "## Setup Perplexity API\n",
    "\n",
    "You will need a key set to `PERPLEXITY_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd41cb9b-1e34-45ed-87b2-67465a964237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "YOUR_API_KEY = os.getenv('PERPLEXITY_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b2815",
   "metadata": {},
   "source": [
    "## Perplexity Helper function\n",
    "\n",
    "No PIP dependency, you **must** have the **OpenAI SDK Installed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5594e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "perplexityClient = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "def generate_text_perplexity(system, user, model):\n",
    "    response = perplexityClient.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d978e",
   "metadata": {},
   "source": [
    "## Test **Perplexity Helper** Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "174e65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate_text_perplexity(\"you are a pirate\", \"say hello and return the message in uppercase\", \"mistral-7b-instruct\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47dfdb",
   "metadata": {},
   "source": [
    "## Setup Anthropic\n",
    "\n",
    "Check the [docs](https://github.com/anthropics/anthropic-sdk-python), and get an [API Key](https://console.anthropic.com/dashboard)\n",
    "\n",
    "### Import SDK \n",
    "\n",
    "PIP Install:\n",
    "\n",
    "`pip install anthropic`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec702c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "anthropic_client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def generate_text_anthropic(user, model=\"claude-3-opus-20240229\"):\n",
    "    response = anthropic_client.messages.create(\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        )\n",
    "    content = response.content[0].text \n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129c756",
   "metadata": {},
   "source": [
    "### Test the Anthropic API directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af97edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate_text_anthropic(\"you are a pirate\" + \"say hello and return the message in uppercase\", \"claude-3-5-sonnet-20240620\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f2438",
   "metadata": {},
   "source": [
    "## Setup Hugging Face\n",
    "\n",
    "### PIP Install Hugging Face Hub\n",
    "\n",
    "`pip install --upgrade huggingface_hub`\n",
    "\n",
    "### Get API Key\n",
    "\n",
    "Head to HuggingFace [Settings Page](https://huggingface.co/settings/tokens) and create an API token.\n",
    "\n",
    "and set it as an environment variable named: `HUGGING_FACE_HUB_TOKEN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5457ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "HF_API_KEY = os.getenv('HUGGING_FACE_HUB_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6631fe",
   "metadata": {},
   "source": [
    "## Hugging Face Helper function using the InferenceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da854c",
   "metadata": {},
   "source": [
    "### Use the InferenceClient to check if a model is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f5e6c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelStatus(loaded=True, state='Loaded', compute_type={'gpu': {'gpu': 'a100', 'count': 8}}, framework='text-generation-inference')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigscience/bloom | bigcode/starcoder\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient()\n",
    "client.get_model_status(\"bigscience/bloom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17aca3",
   "metadata": {},
   "source": [
    "### Must Enable Models In Hugging Face to use them \n",
    "\n",
    "**Note** - to use HF models (which can be an URL to a private model, or a `model_id`) you will need to load that [model](https://huggingface.co/models) into your Hugging Face profile first.\n",
    "\n",
    "Some models are available without enabling them. The first models includes `bloom` which is already enabling, and `gemma7b` which is one that requires to enable it first before using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21f26589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient, InferenceTimeoutError\n",
    "\n",
    "def generate_text_huggingface(user, model=\"\"):\n",
    "    try:\n",
    "        huggingface_client = InferenceClient(model=model, token=HF_API_KEY, timeout=60)\n",
    "        response = huggingface_client.text_generation(user, max_new_tokens=1024)\n",
    "    except InferenceTimeoutError:\n",
    "        print(\"Inference timed out after 60s.\")\n",
    "    return response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8ff78",
   "metadata": {},
   "source": [
    "### Test the Hugging Face Helper function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ba271a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate_text_huggingface(\"you are a pirate tell me your favorite color\", \"meta-llama/Llama-2-7b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7037e5",
   "metadata": {},
   "source": [
    "## Setup NVidia NGC\n",
    "\n",
    "You must go to (NVidia Builder Portal)[https://build.nvidia.com/] to setup an API key to use their models.\n",
    "\n",
    "For this function you **must** have the **Open AI SDK Installed** since there is no SDK for NVidia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "582cd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "NVIDIA_API_KEY = os.getenv('NVIDIA_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed60801",
   "metadata": {},
   "source": [
    "### Helper function for using NVidia Inference APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbae66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = NVIDIA_API_KEY\n",
    ")\n",
    "\n",
    "def generate_text_ngc(system, user, model):\n",
    "  completion = nvidia_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "      {\"role\":\"system\",\"content\":system},\n",
    "      {\"role\":\"user\",\"content\":user},\n",
    "      \n",
    "    ],\n",
    "    max_tokens=1024\n",
    "  )\n",
    "\n",
    "  content = completion.choices[0].message.content\n",
    "  return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566961e2",
   "metadata": {},
   "source": [
    "### Test for the NVidia function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef443be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38578a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate_text_ngc(\"you are a pirate\", \"you are a pirate tell me your favorite color\", \"meta/llama3-8b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e643db",
   "metadata": {},
   "source": [
    "## Add Actions to map to different models and AI providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aa848",
   "metadata": {},
   "source": [
    "1. Define a function for each model you want to test\n",
    "2. Create a constant to reference that model\n",
    "3. Add both to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "282d025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the common interface for all the models\n",
    "# It takes the **system** message, **user** message and the **output style** instructions and calls\n",
    "# the model specific function with those inputs (matching the API signature)\n",
    "# Constants for the models - this name is arbitrary, should be unique\n",
    "ANTHROPIC_OPUS = \"claude-3-opus-20240229\"\n",
    "ANTHROPIC_SONNET = \"claude-3-5-sonnet-20240620\"\n",
    "GEMINI_PRO = \"gemini-1.0-pro-latest\"\n",
    "GEMINI_FLASH = \"gemini-1.5-flash-latest\"\n",
    "HUGGINGFACE_BLOOM = \"bigscience/bloom\"\n",
    "HUGGINGFACE_GEMMA7B = \"google/gemma-7b\"\n",
    "HUGGINGFACE_LLAMA2_7B = \"meta-llama/Llama-2-7b\"\n",
    "OPEN_AI_GPT35TURBO = \"gpt-3.5-turbo\"\n",
    "OPEN_AI_GPT4 = \"gpt-4\"\n",
    "OPEN_AI_GPT4O = \"gpt-4o\"\n",
    "OPEN_AI_GPT4PREVIEW = \"gpt-4-0125-preview\"\n",
    "MISTRAL_7B = \"mistral-7b-instruct\"\n",
    "MIXTRAL_8X7B = \"mixtral-8x7b-instruct\"\n",
    "NVIDIA_LLAMA3_8B = \"meta/llama3-8b\"\n",
    "NVIDIA_LLAMA3_70B = \"meta/llama3-70b\"\n",
    "SONAR_MED_ONLINE = \"sonar-medium-online\"\n",
    "\n",
    "def action_anthropic_opus(system, user, output_style):\n",
    "    response = generate_text_anthropic(system + user + output_style, ANTHROPIC_OPUS)\n",
    "    return response\n",
    "\n",
    "def action_anthropic_sonnet(system, user, output_style):\n",
    "    response = generate_text_anthropic(system + user + output_style, ANTHROPIC_SONNET)\n",
    "    return response\n",
    "\n",
    "def action_gemini_pro(system, user, output_style,):\n",
    "    response = generate_text_google(system + user + output_style, GEMINI_PRO)\n",
    "    return response\n",
    "\n",
    "def action_gemini_flash(system, user, output_style,):\n",
    "    response = generate_text_google(system + user + output_style, GEMINI_FLASH)\n",
    "    return response\n",
    "\n",
    "def action_huggingface_bloom(system, user, output_style,):\n",
    "    response = generate_text_huggingface(system + user + output_style, HUGGINGFACE_BLOOM)\n",
    "    return response\n",
    "\n",
    "def action_huggingface_gemma7b(system, user, output_style,):\n",
    "    response = generate_text_huggingface(system + user + output_style, HUGGINGFACE_GEMMA7B)\n",
    "    return response\n",
    "\n",
    "def action_huggingface_llama2_7b(system, user, output_style,):\n",
    "    response = generate_text_huggingface(system + user + output_style, HUGGINGFACE_LLAMA2_7B)\n",
    "    return response\n",
    "\n",
    "def action_openai_35turbo(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, OPEN_AI_GPT35TURBO)\n",
    "    return response\n",
    "\n",
    "def action_openai_gpt4(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, OPEN_AI_GPT4)\n",
    "    return response\n",
    "\n",
    "def action_openai_gpt4o(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, OPEN_AI_GPT4O)\n",
    "    return response\n",
    "\n",
    "def action_openai_gpt4_preview(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, OPEN_AI_GPT4PREVIEW)\n",
    "    return response\n",
    "\n",
    "def action_mistral_7b(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, MISTRAL_7B)\n",
    "    return response\n",
    "\n",
    "def action_mixtral_8x7b(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, MIXTRAL_8X7B)\n",
    "    return response\n",
    "\n",
    "def action_nvidia_llama3_8b(system, user, output_style):\n",
    "    response = generate_text_ngc(system, user + output_style, NVIDIA_LLAMA3_8B)\n",
    "    return response\n",
    "\n",
    "def action_nvidia_llama3_70b(system, user, output_style):\n",
    "    response = generate_text_ngc(system, user + output_style, NVIDIA_LLAMA3_70B)\n",
    "    return response\n",
    "\n",
    "def action_sonar_medium_online(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, SONAR_MED_ONLINE)\n",
    "    return response\n",
    "\n",
    "# Dictionary mapping models to their respective functions\n",
    "action_dict = {\n",
    "    ANTHROPIC_OPUS: action_anthropic_opus,\n",
    "    ANTHROPIC_SONNET: action_anthropic_sonnet,\n",
    "    GEMINI_PRO: action_gemini_pro,\n",
    "    GEMINI_FLASH: action_gemini_flash,\n",
    "    HUGGINGFACE_BLOOM: action_huggingface_bloom,\n",
    "    HUGGINGFACE_GEMMA7B: action_huggingface_gemma7b,\n",
    "    HUGGINGFACE_LLAMA2_7B: action_huggingface_llama2_7b,\n",
    "    OPEN_AI_GPT35TURBO: action_openai_35turbo,\n",
    "    OPEN_AI_GPT4: action_openai_gpt4,\n",
    "    OPEN_AI_GPT4O: action_openai_gpt4o,\n",
    "    OPEN_AI_GPT4PREVIEW: action_openai_gpt4_preview,\n",
    "    MISTRAL_7B: action_mistral_7b,\n",
    "    MIXTRAL_8X7B: action_mixtral_8x7b,\n",
    "    NVIDIA_LLAMA3_8B: action_nvidia_llama3_8b,\n",
    "    NVIDIA_LLAMA3_70B: action_nvidia_llama3_70b,\n",
    "    SONAR_MED_ONLINE: action_sonar_medium_online\n",
    "}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e2bf2",
   "metadata": {},
   "source": [
    "## Main Entry Point to call appropriate functions based which are requested in `models` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c6ecd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(models, system, user, output_style):\n",
    "    \"\"\"\n",
    "    Generate text responses from multiple AIs based on **models** in list.\n",
    "\n",
    "    If there is only 1 models in the list, the response will not include the model name.\n",
    "    Otherwise, the response will include the model name as a header of the text generated from each model.\n",
    "\n",
    "    Args:\n",
    "        models (list): A list of model names indicating which ones to run.\n",
    "        system (str): The prompt *system* information to define context.\n",
    "        user (str): The prompt *user* information to describe the question to ask.\n",
    "        output_style (str): The prompt desired *output_style* of the generated text.\n",
    "\n",
    "    Returns:\n",
    "        str: the generated text for all of the models in the input list\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    is_single_model = len(models) == 1\n",
    "\n",
    "    for model in models:\n",
    "        action = action_dict.get(model)\n",
    "        if action:\n",
    "            try:\n",
    "                response = action(system=system, user=user, output_style=output_style)\n",
    "                if not is_single_model:\n",
    "                    output += \"\\n\\n# MODEL: \" + model + \"\\n\"\n",
    "                output += response\n",
    "            except Exception as e:\n",
    "                if not is_single_model:\n",
    "                    output += \"\\n\\n# MODEL: \" + model + \"\\n\"\n",
    "                output += \"Exception\" + str(e)\n",
    "        else:\n",
    "            print(\"No action defined for model: \", model)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080f61d",
   "metadata": {},
   "source": [
    "## Final Step\n",
    "\n",
    "After making changes to this notebook, run the following on the command-line to create the python script to use:\n",
    "\n",
    "```\n",
    "jupyter nbconvert --to script .\\multi_ai_hub.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159491e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
