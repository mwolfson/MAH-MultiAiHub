{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e564a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94215c9f",
   "metadata": {},
   "source": [
    "## MAH - Multi AI Hub\n",
    "\n",
    "This project is designed to make it easy to send the same prompt to multiple LLMs which is useful for testing and comparison.\n",
    "\n",
    "### API Access Required\n",
    "\n",
    "You must have access to the services (Currently Anthropic, Google, OpenAI, and Perplexity) in order to use them in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fde748",
   "metadata": {},
   "source": [
    "### Working with API keys\n",
    "\n",
    "Set the API keys as a system variables.\n",
    "\n",
    "- [Setting an Environment Variable on Mac/Linux](https://phoenixnap.com/kb/set-environment-variable-mac)\n",
    "- [Setting an Environment Variable on Windows](https://phoenixnap.com/kb/windows-set-environment-variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c7f07",
   "metadata": {},
   "source": [
    "## Tools to Get Environment Variables from OS\n",
    "\n",
    "PIP Install:\n",
    "\n",
    "`pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d788df",
   "metadata": {},
   "source": [
    "## Adding other models\n",
    "\n",
    "### Check for Provider *Helper* Function\n",
    "\n",
    "This is organized into API providers, there are helper functions for:\n",
    "- [**Anthropic**](#anthropic_api) | [API Docs](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)\n",
    "- [**Google**](#google_api) | [API Docs](https://ai.google.dev/)\n",
    "- [**OpenAI**](#openai_api) | [API Docs](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)\n",
    "- [**Perplexity**](#pplx_api) | [API Docs](https://docs.perplexity.ai/)\n",
    "\n",
    "Create a new helper function if necessary, then skip to the bottom, and add your calls to the Action dictionary, where these are mapped (pretty simple)\n",
    "\n",
    "Happy Model Comparing!\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85524dc3",
   "metadata": {},
   "source": [
    "## <a name=\"google_api\"></a>Setup Google GenAI\n",
    "\n",
    "### Import Google Generative GenerativeAI library and set API Key\n",
    "\n",
    "PIP Install: \n",
    "\n",
    "`pip install -q google.generativeai`\n",
    "\n",
    "You will need to set the Gemini API key as a system variable named: `GOOGLE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27165c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as googleai\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "apiKey = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "googleai.configure(api_key=apiKey,\n",
    "               transport=\"rest\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4284ec8",
   "metadata": {},
   "source": [
    "## Customize Gemini Settings\n",
    "\n",
    "Use `generation_config` to specify various things (Ex. `temperature`, and `max_output_tokens`)\n",
    "\n",
    "Use `safety_settings` to check the output to ensure it is free of harmful language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a2c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold, GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=4096\n",
    ")\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20290bb8",
   "metadata": {},
   "source": [
    "## Explore the Available Models\n",
    "\n",
    "Learn which models are currently available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f805008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in googleai.list_models():\n",
    "#     print(f\"name: {m.name}\")\n",
    "#     print(f\"description: {m.description}\")\n",
    "#     print(f\"generation methods:{m.supported_generation_methods}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e486139",
   "metadata": {},
   "source": [
    "### Filter models to ensure model we want is supported\n",
    "- `generateContent` is the value we are looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d50897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in googleai.list_models():\n",
    "#   if 'generateContent' in m.supported_generation_methods:\n",
    "#     print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3b4ba",
   "metadata": {},
   "source": [
    "### Google AI Helper Function\n",
    "\n",
    "- The `@retry` decorator helps you to retry the API call if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8738a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "@retry.Retry()\n",
    "def generate_text_google(prompt, model):\n",
    "    model = googleai.GenerativeModel(\n",
    "        model_name=model,\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings\n",
    "        )\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb2c16",
   "metadata": {},
   "source": [
    "### Test **Google AI Helper** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate_text_google(\"Thursday evenings are perfect for\", \"gemini-1.5-flash-latest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0bb07",
   "metadata": {},
   "source": [
    "## <a name=\"openai_api\"></a>Setup Open AI APIs\n",
    "\n",
    "```\n",
    "OpenAI's APIs offer developers the ability to integrate advanced artificial intelligence capabilities into their applications, enabling a wide range of tasks from text generation to complex problem-solving.\n",
    "```\n",
    "Documentation: [https://beta.openai.com/docs/](https://beta.openai.com/docs/)\n",
    "\n",
    "### Obtaining API Keys:\n",
    "- **OpenAI Platform**: [https://platform.openai.com/](https://platform.openai.com/)\n",
    "  - After signing up or logging in, navigate to the API section to manage and obtain your API keys.\n",
    "- You will need to set the OpenAI API key as a system variable named: `OPENAI_API_KEY`.  \n",
    "\n",
    "Note: do NOT check your API key into a public Github repo, or it will get revoked \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17cf19-f3ee-4ccb-95fe-afbf2d592c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ecbad",
   "metadata": {},
   "source": [
    "### Open AI Helper Function\n",
    "\n",
    "PIP Dependencies:\n",
    "\n",
    "`pip install --upgrade openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa30282-118b-499e-978e-39e832214847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_text_openai(pre, prompt, model):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": pre},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39685df4",
   "metadata": {},
   "source": [
    "## Test **Open AI Helper** Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate_text_openai(\"You are a pirate\", \"Thursday evenings are perfect for\", \"gpt-4o\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e34e4",
   "metadata": {},
   "source": [
    "## <a name=\"pplx_api\"></a>Setup Perplexity API\n",
    "\n",
    "You will need a key set to `PERPLEXITY_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41cb9b-1e34-45ed-87b2-67465a964237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "YOUR_API_KEY = os.getenv('PERPLEXITY_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b2815",
   "metadata": {},
   "source": [
    "## Perplexity Helper function\n",
    "\n",
    "No PIP dependency, you **must** have the **OpenAI SDK Installed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "perplexityClient = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "def generate_text_perplexity(system, user, model):\n",
    "    response = perplexityClient.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d978e",
   "metadata": {},
   "source": [
    "## Test **Perplexity Helper** Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate_text_perplexity(\"you are a pirate\", \"say hello and return the message in uppercase\", \"mistral-7b-instruct\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47dfdb",
   "metadata": {},
   "source": [
    "## <a name=\"anthropic_api\"></a>Setup Anthropic\n",
    "\n",
    "Check the [docs](https://github.com/anthropics/anthropic-sdk-python), and get an [API Key](https://console.anthropic.com/dashboard)\n",
    "\n",
    "### Import SDK \n",
    "\n",
    "PIP Install:\n",
    "\n",
    "`pip install anthropic`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec702c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "anthropic_client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def generate_text_anthropic(user, model=\"claude-3-opus-20240229\"):\n",
    "    response = anthropic_client.messages.create(\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        )\n",
    "    content = response.content[0].text \n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129c756",
   "metadata": {},
   "source": [
    "### Test the Anthropic API directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generate_text_anthropic(\"you are a pirate\" + \"say hello and return the message in uppercase\", \"claude-3-opus-20240229\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65d052",
   "metadata": {},
   "source": [
    "## <a name=\"azure_api\"></a>Setup Azure\n",
    "\n",
    "Check the [docs](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal), and get a project setup.\n",
    "\n",
    "You will need an Project URI and an API_KEY and you should create environment variables for these, with the following names:\n",
    "\n",
    "- AZURE_ENDPOINT_URL\n",
    "- AZURE_OPENAI_API_KEY\n",
    "\n",
    "### Import SDK \n",
    "\n",
    "There is no additional dependencies, because this uses the OpenAI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "endpoint = os.getenv('AZURE_ENDPOINT_URL')\n",
    "apiKey = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "      \n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=apiKey,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "def generate_text_azure(pre, prompt, model=\"gpt-4\"):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": pre},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5d510",
   "metadata": {},
   "source": [
    "### Test the Azure Endpoint directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generate_text_azure(\"you are a pirate\", \"say hello and return the message in uppercase\", \"gpt-4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e643db",
   "metadata": {},
   "source": [
    "## Add Actions to map to different models and AI providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aa848",
   "metadata": {},
   "source": [
    "1. Define a function for each model you want to test\n",
    "2. Create a constant to reference that model\n",
    "3. Add both to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the models - use the unique name of the model as defined in the SDK\n",
    "ANTHROPIC_OPUS = \"claude-3-opus-20240229\"\n",
    "ANTHROPIC_SONNET = \"claude-3-5-sonnet-20240620\"\n",
    "AZURE_GPT4 = \"gpt-4\"\n",
    "GEMINI_PRO = \"gemini-pro\"\n",
    "GEMINI_FLASH = \"gemini-1.5-flash-latest\"\n",
    "OPEN_AI_GPT35TURBO = \"gpt-3.5-turbo\"\n",
    "OPEN_AI_GPT4 = \"gpt-4\"\n",
    "OPEN_AI_GPT4O = \"gpt-4o\"\n",
    "OPEN_AI_GPT4PREVIEW = \"gpt-4-0125-preview\"\n",
    "PPLX_LLAMA3_8B = \"llama-3-8b-instruct\"\n",
    "PPLX_LLAMA3_70B = \"llama-3-70b-instruct\"\n",
    "PPLX_MISTRAL_7B = \"mistral-7b-instruct\"\n",
    "PPLX_MIXTRAL_8X7B = \"mixtral-8x7b-instruct\"\n",
    "SONAR_MED_ONLINE = \"sonar-medium-online\"\n",
    "\n",
    "# This is the common interface for all the models\n",
    "# It takes the **system** message, **user** message and the **output style** instructions and calls\n",
    "# the model specific function with those inputs (matching the API signature)\n",
    "def action_anthropic_opus(system, user, output_style):\n",
    "    response = generate_text_anthropic(system + user + output_style, ANTHROPIC_OPUS)\n",
    "    return response\n",
    "\n",
    "def action_anthropic_sonnet(system, user, output_style):\n",
    "    response = generate_text_anthropic(system + user + output_style, ANTHROPIC_SONNET)\n",
    "    return response\n",
    "\n",
    "def action_azure_gpt4(system, user, output_style):\n",
    "    response = generate_text_azure(system, user + output_style, AZURE_GPT4)\n",
    "    return response\n",
    "\n",
    "def action_gemini_pro(system, user, output_style,):\n",
    "    response = generate_text_google(system + user + output_style, GEMINI_PRO)\n",
    "    return response\n",
    "\n",
    "def action_gemini_flash(system, user, output_style,):\n",
    "    response = generate_text_google(system + user + output_style, GEMINI_FLASH)\n",
    "    return response\n",
    "\n",
    "def action_openai_35turbo(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, OPEN_AI_GPT35TURBO)\n",
    "    return response\n",
    "\n",
    "def action_openai_gpt4(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, OPEN_AI_GPT4)\n",
    "    return response\n",
    "\n",
    "def action_openai_gpt4o(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, OPEN_AI_GPT4O)\n",
    "    return response\n",
    "\n",
    "def action_openai_gpt4_preview(system, user, output_style):\n",
    "    response = generate_text_openai(system, user + output_style, OPEN_AI_GPT4PREVIEW)\n",
    "    return response\n",
    "\n",
    "def action_pplxllama_8b(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, PPLX_LLAMA3_8B)\n",
    "    return response\n",
    "\n",
    "def action_pplxllama_70b(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, PPLX_LLAMA3_70B)\n",
    "    return response\n",
    "\n",
    "def action_pplxmistral_7b(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, PPLX_MISTRAL_7B)\n",
    "    return response\n",
    "\n",
    "def action_pplxmixtral_8x7b(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, PPLX_MIXTRAL_8X7B)\n",
    "    return response\n",
    "\n",
    "def action_sonar_medium_online(system, user, output_style):\n",
    "    response = generate_text_perplexity(system, user + output_style, SONAR_MED_ONLINE)\n",
    "    return response\n",
    "\n",
    "# Dictionary mapping models to their respective functions will be used by the client\n",
    "action_dict = {\n",
    "    ANTHROPIC_OPUS: action_anthropic_opus,\n",
    "    ANTHROPIC_SONNET: action_anthropic_sonnet,\n",
    "    AZURE_GPT4: action_azure_gpt4,\n",
    "    GEMINI_PRO: action_gemini_pro,\n",
    "    GEMINI_FLASH: action_gemini_flash,\n",
    "    OPEN_AI_GPT35TURBO: action_openai_35turbo,\n",
    "    OPEN_AI_GPT4: action_openai_gpt4,\n",
    "    OPEN_AI_GPT4O: action_openai_gpt4o,\n",
    "    OPEN_AI_GPT4PREVIEW: action_openai_gpt4_preview,\n",
    "    PPLX_LLAMA3_8B: action_pplxllama_8b,\n",
    "    PPLX_LLAMA3_70B: action_pplxllama_70b,\n",
    "    PPLX_MISTRAL_7B: action_pplxmistral_7b,\n",
    "    PPLX_MIXTRAL_8X7B: action_pplxmixtral_8x7b,\n",
    "    SONAR_MED_ONLINE: action_sonar_medium_online\n",
    "}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e2bf2",
   "metadata": {},
   "source": [
    "## Main Entry Point to call appropriate functions based which are requested in `models` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ecd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(models, system, user, output_style):\n",
    "    \"\"\"\n",
    "    Generate text responses from multiple AIs based on **models** in list.\n",
    "\n",
    "    If there is only 1 models in the list, the response will not include the model name.\n",
    "    Otherwise, the response will include the model name as a header of the text generated from each model.\n",
    "\n",
    "    Args:\n",
    "        models (list): A list of model names indicating which ones to run.\n",
    "        system (str): The prompt *system* information to define context.\n",
    "        user (str): The prompt *user* information to describe the question to ask.\n",
    "        output_style (str): The prompt desired *output_style* of the generated text.\n",
    "\n",
    "    Returns:\n",
    "        str: the generated text for all of the models in the input list\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    is_single_model = len(models) == 1\n",
    "\n",
    "    for model in models:\n",
    "        action = action_dict.get(model)\n",
    "        if action:\n",
    "            try:\n",
    "                response = action(system=system, user=user, output_style=output_style)\n",
    "                if not is_single_model:\n",
    "                    output += \"\\n\\n# MODEL: \" + model + \"\\n\"\n",
    "                output += response\n",
    "            except Exception as e:\n",
    "                if not is_single_model:\n",
    "                    output += \"\\n\\n# MODEL: \" + model + \"\\n\"\n",
    "                output += \"Exception\" + str(e)\n",
    "        else:\n",
    "            print(\"No action defined for model: \", model)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14db010",
   "metadata": {},
   "source": [
    "## Test Method\n",
    "\n",
    "This method will run the MAH with all the models you want to test...this is convenient to check if all the inference calls work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [    \n",
    "#     ANTHROPIC_OPUS,\n",
    "#     AZURE_GPT4\n",
    "# ]\n",
    "\n",
    "# system = \"You are a pirate\"\n",
    "# user = \"Make a greeting and tell me a joke about treasure\"\n",
    "# output_style = \"Output the response in all capital letters\"\n",
    "\n",
    "# response = generate_text(models, system, user, output_style)\n",
    "\n",
    "# # Output the response to the console\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080f61d",
   "metadata": {},
   "source": [
    "## Final Step\n",
    "\n",
    "After making changes to this notebook, run the following on the command-line to create the python script to use:\n",
    "\n",
    "```\n",
    "jupyter nbconvert --to script .\\multi_ai_hub.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509ce7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159491e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
